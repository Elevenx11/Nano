# neural_response_engine.py - Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ Ù„Ù„Ø±Ø¯ÙˆØ¯
import json
import random
import re
import math
from typing import Dict, List, Tuple, Optional, Set
from collections import defaultdict, Counter, deque
from dataclasses import dataclass
from datetime import datetime
import pickle
import os

@dataclass
class ResponsePattern:
    """Ù†Ù…Ø· Ø§Ù„Ø±Ø¯"""
    input_pattern: List[str]
    response_template: str
    success_rate: float
    context_type: str
    emotion_trigger: str
    usage_count: int
    last_used: datetime

@dataclass
class ConversationContext:
    """Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
    previous_messages: List[str]
    current_emotion: str
    user_personality_type: str
    relationship_level: float  # Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ù…Ù† 0 Ø¥Ù„Ù‰ 1
    conversation_topic: str

class NeuralResponseEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ Ù„Ù„Ø±Ø¯ÙˆØ¯ - ÙŠØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©"""
    
    def __init__(self, data_path: str = "data"):
        self.data_path = data_path
        
        # Ù‚Ø§Ø¹Ø¯Ø© Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©
        self.response_patterns = []
        self.conversation_contexts = deque(maxlen=200)
        self.successful_interactions = deque(maxlen=500)
        
        # Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù…
        self.word_associations = defaultdict(lambda: defaultdict(float))
        self.phrase_templates = defaultdict(list)
        self.context_response_map = defaultdict(lambda: defaultdict(float))
        
        # Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
        self.response_generators = {
            "markov_chain": self.generate_markov_response,
            "associative": self.generate_associative_response,
            "contextual": self.generate_contextual_response
        }
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù…
        self.learning_metrics = {
            "total_patterns": 0,
            "success_rate": 0.0,
            "diversity_score": 0.0,
            "last_training": None
        }
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        self.initialize_base_patterns()
        self.load_learned_data()

    def initialize_base_patterns(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø¨Ø¯Ø§ÙŠØ©"""
        
        # Ø£Ù†Ù…Ø§Ø· Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„ØªØ¹Ù„Ù… Ù…Ù†Ù‡Ø§
        base_patterns = [
            {
                "input": ["Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "Ù…Ø±Ø­Ø¨Ø§", "Ø£Ù‡Ù„Ø§", "Ù‡Ø§ÙŠ"],
                "response_type": "greeting",
                "templates": ["ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø­Ø¨ÙŠØ¨ÙŠ", "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹", "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙŠØ§ ØºØ§Ù„ÙŠ"],
                "emotion": "ÙˆØ¯"
            },
            {
                "input": ["ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ", "Ø´Ù„ÙˆÙ†Ùƒ", "Ø¥ÙŠØ´ Ø£Ø®Ø¨Ø§Ø±Ùƒ"],
                "response_type": "status_check", 
                "templates": ["Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ ØªÙ…Ø§Ù…", "Ø¨Ø®ÙŠØ± ÙˆØ§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡", "ÙƒÙ„Ù‡ Ø²ÙŠÙ†"],
                "emotion": "Ø·ÙŠØ¨"
            },
            {
                "input": ["Ø´ÙƒØ±Ø§Ù‹", "ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©", "ØªØ³Ù„Ù…"],
                "response_type": "gratitude_response",
                "templates": ["Ø§Ù„Ø¹ÙÙˆ Ø­Ø¨ÙŠØ¨ÙŠ", "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø§ÙÙŠÙƒ", "Ù…Ø§ Ø³ÙˆÙŠØª Ø´ÙŠ"],
                "emotion": "ØªÙ‚Ø¯ÙŠØ±"
            }
        ]
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„Ù…
        for pattern in base_patterns:
            for template in pattern["templates"]:
                self.add_successful_pattern(
                    input_sample=" ".join(pattern["input"][:2]),
                    response=template,
                    context_type=pattern["response_type"],
                    emotion=pattern["emotion"]
                )

    def add_successful_pattern(self, input_sample: str, response: str, 
                             context_type: str, emotion: str, success_score: float = 1.0):
        """Ø¥Ø¶Ø§ÙØ© Ù†Ù…Ø· Ø±Ø¯ Ù†Ø§Ø¬Ø­ Ù„Ù„ØªØ¹Ù„Ù…"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        input_words = self.extract_keywords(input_sample)
        response_words = response.split()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø· Ø§Ù„Ø±Ø¯
        pattern = ResponsePattern(
            input_pattern=input_words,
            response_template=response,
            success_rate=success_score,
            context_type=context_type,
            emotion_trigger=emotion,
            usage_count=1,
            last_used=datetime.now()
        )
        
        self.response_patterns.append(pattern)
        
        # ØªØ­Ø¯ÙŠØ« Ø®Ø±Ø§Ø¦Ø· Ø§Ù„ØªØ¹Ù„Ù…
        self.update_word_associations(input_words, response_words, success_score)
        self.update_context_mappings(context_type, emotion, response, success_score)
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ù†Ø§Ø¬Ø­
        self.successful_interactions.append({
            'input': input_sample,
            'response': response, 
            'context': context_type,
            'emotion': emotion,
            'success_score': success_score,
            'timestamp': datetime.now()
        })

    def extract_keywords(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø©"""
        
        # ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ù…Ù‡Ù…Ø© Ù„ØªØ¬Ø§Ù‡Ù„Ù‡Ø§
        stop_words = {
            "ÙÙŠ", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù„Ù‰", "Ø¹Ù†", "Ù…Ø¹", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ",
            "Ø§Ù„ØªÙŠ", "Ø§Ù„ØªÙŠ", "Ø§Ù„Ù„ÙŠ", "Ø§Ù„ÙŠ", "ÙˆØ´", "Ø¥ÙŠØ´", "Ù„ÙŠØ´", "ÙƒÙŠÙ"
        }
        
        words = re.findall(r'\w+', text.lower())
        keywords = [w for w in words if w not in stop_words and len(w) > 2]
        
        return keywords[:5]  # Ø£Ù‡Ù… 5 ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©

    def update_word_associations(self, input_words: List[str], response_words: List[str], 
                                success_score: float):
        """ØªØ­Ø¯ÙŠØ« ØªØ±Ø§Ø¨Ø·Ø§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª"""
        
        for input_word in input_words:
            for response_word in response_words:
                # ØªÙ‚ÙˆÙŠØ© Ø§Ù„ØªØ±Ø§Ø¨Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¬Ø§Ø­
                self.word_associations[input_word][response_word] += 0.1 * success_score
                
                # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙ… Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ù†Ù…Ùˆ Ø§Ù„Ù…ÙØ±Ø·
                if self.word_associations[input_word][response_word] > 1.0:
                    self.word_associations[input_word][response_word] = 1.0

    def update_context_mappings(self, context_type: str, emotion: str, 
                               response: str, success_score: float):
        """ØªØ­Ø¯ÙŠØ« Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø±"""
        
        context_key = f"{context_type}_{emotion}"
        
        # ØªØ­Ø¯ÙŠØ« Ù‚ÙˆØ© Ø§Ù„Ø±Ø¯ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚
        self.context_response_map[context_key][response] += 0.1 * success_score

    def generate_smart_response(self, user_input: str, emotion: str = "Ù…Ø­Ø§ÙŠØ¯", 
                               context: ConversationContext = None) -> Tuple[str, float, str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø°ÙƒÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù…"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø¯Ø®Ù„
        input_keywords = self.extract_keywords(user_input)
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· Ù…Ø´Ø§Ø¨Ù‡Ø©
        similar_patterns = self.find_similar_patterns(input_keywords, emotion)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ÙˆØ¯ Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨Ø·Ø±Ù‚ Ù…Ø®ØªÙ„ÙØ©
        candidate_responses = []
        
        # 1. Ù…Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©
        if similar_patterns:
            pattern_response = self.generate_from_patterns(similar_patterns, context)
            candidate_responses.append((pattern_response, "pattern_based", 0.8))
        
        # 2. Ù…Ù† Ø³Ù„Ø§Ø³Ù„ Ù…Ø§Ø±ÙƒÙˆÙ
        markov_response = self.generate_markov_response(input_keywords, emotion)
        candidate_responses.append((markov_response, "markov", 0.6))
        
        # 3. Ù…Ù† Ø§Ù„ØªØ±Ø§Ø¨Ø·Ø§Øª
        associative_response = self.generate_associative_response(input_keywords)
        candidate_responses.append((associative_response, "associative", 0.5))
        
        # 4. Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©
        if context:
            contextual_response = self.generate_contextual_response(input_keywords, context)
            candidate_responses.append((contextual_response, "contextual", 0.7))
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø±Ø¯
        best_response, method, confidence = self.select_best_response(candidate_responses, user_input)
        
        return best_response, confidence, method

    def find_similar_patterns(self, input_keywords: List[str], emotion: str) -> List[ResponsePattern]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ù†Ù…Ø§Ø· Ù…Ø´Ø§Ø¨Ù‡Ø©"""
        
        similar_patterns = []
        
        for pattern in self.response_patterns:
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
            similarity = self.calculate_pattern_similarity(input_keywords, pattern.input_pattern)
            
            # ØªØ¹Ø¯ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
            if pattern.emotion_trigger == emotion:
                similarity += 0.2
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©
            if similarity > 0.3:  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ´Ø§Ø¨Ù‡
                similar_patterns.append((pattern, similarity))
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆÙ…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­
        similar_patterns.sort(key=lambda x: x[1] * x[0].success_rate, reverse=True)
        
        return [pattern for pattern, score in similar_patterns[:5]]

    def calculate_pattern_similarity(self, keywords1: List[str], keywords2: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù…Ø¬Ù…ÙˆØ¹ØªÙŠÙ† Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        
        if not keywords1 or not keywords2:
            return 0.0
        
        set1 = set(keywords1)
        set2 = set(keywords2)
        
        # ØªØ´Ø§Ø¨Ù‡ Ø¬Ø§ÙƒØ§Ø±Ø¯
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        return intersection / union if union > 0 else 0.0

    def generate_from_patterns(self, patterns: List[ResponsePattern], 
                              context: ConversationContext = None) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ù…Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©"""
        
        if not patterns:
            return self.generate_fallback_response()
        
        # Ø§Ø®ØªÙŠØ§Ø± Ù†Ù…Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¬Ø§Ø­ ÙˆØ§Ù„Ø­Ø¯Ø§Ø«Ø©
        weights = []
        for pattern in patterns:
            # ÙˆØ²Ù† ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ ÙˆÙ…Ø¯Ù‰ Ø§Ù„Ø­Ø¯Ø§Ø«Ø©
            recency_factor = 1.0 - ((datetime.now() - pattern.last_used).days / 30.0)
            recency_factor = max(0.1, recency_factor)
            
            weight = pattern.success_rate * recency_factor
            weights.append(weight)
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù…Ø±Ø¬Ø­
        selected_pattern = random.choices(patterns, weights=weights)[0]
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        selected_pattern.usage_count += 1
        selected_pattern.last_used = datetime.now()
        
        # Ø¥Ø¶Ø§ÙØ© ØªÙ†ÙˆÙŠØ¹ Ù„Ù„Ø±Ø¯
        return self.add_variation_to_response(selected_pattern.response_template)

    def generate_markov_response(self, keywords: List[str], emotion: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø³Ù„Ø§Ø³Ù„ Ù…Ø§Ø±ÙƒÙˆÙ"""
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…Ù„ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø©
        relevant_responses = []
        
        for interaction in self.successful_interactions:
            interaction_keywords = self.extract_keywords(interaction['input'])
            
            # ÙØ­Øµ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
            if any(kw in interaction_keywords for kw in keywords):
                relevant_responses.append(interaction['response'])
        
        if not relevant_responses:
            return self.generate_fallback_response()
        
        # Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø§Ø±ÙƒÙˆÙ Ø¨Ø³ÙŠØ·
        words = []
        for response in relevant_responses:
            words.extend(response.split())
        
        if len(words) < 3:
            return random.choice(relevant_responses)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø¬Ø¯ÙŠØ¯
        generated_words = []
        current_word = random.choice(words)
        generated_words.append(current_word)
        
        for _ in range(random.randint(3, 8)):
            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªØ£ØªÙŠ Ø¨Ø¹Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            next_words = []
            for i, word in enumerate(words[:-1]):
                if word == current_word:
                    next_words.append(words[i + 1])
            
            if next_words:
                current_word = random.choice(next_words)
                generated_words.append(current_word)
            else:
                break
        
        return " ".join(generated_words)

    def generate_associative_response(self, keywords: List[str]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø§Ø¨Ø·Ø§Øª"""
        
        response_words = []
        
        for keyword in keywords:
            if keyword in self.word_associations:
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‚ÙˆÙ‰ Ø§Ù„ØªØ±Ø§Ø¨Ø·Ø§Øª
                associations = self.word_associations[keyword]
                if associations:
                    # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ 3 ÙƒÙ„Ù…Ø§Øª Ù…Ø±ØªØ¨Ø·Ø©
                    top_words = sorted(associations.items(), key=lambda x: x[1], reverse=True)[:3]
                    response_words.extend([word for word, score in top_words])
        
        if not response_words:
            return self.generate_fallback_response()
        
        # Ø¨Ù†Ø§Ø¡ Ø±Ø¯ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©
        unique_words = list(set(response_words))[:6]
        
        # Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ø±Ø¨Ø· Ø¨Ø³ÙŠØ·Ø©
        connectors = ["Ùˆ", "Ù„ÙƒÙ†", "ÙƒØ°Ù„Ùƒ", "Ø£ÙŠØ¶Ø§Ù‹", ""]
        
        # Ø¨Ù†Ø§Ø¡ Ø¬Ù…Ù„Ø© Ø¨Ø³ÙŠØ·Ø©
        if len(unique_words) >= 2:
            connector = random.choice(connectors)
            if connector:
                return f"{unique_words[0]} {connector} {' '.join(unique_words[1:])}"
            else:
                return " ".join(unique_words)
        else:
            return unique_words[0] if unique_words else self.generate_fallback_response()

    def generate_contextual_response(self, keywords: List[str], context: ConversationContext) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø³ÙŠØ§Ù‚ÙŠ Ù…ØªØ·ÙˆØ±"""
        
        context_key = f"{context.conversation_topic}_{context.current_emotion}"
        
        if context_key in self.context_response_map:
            responses = self.context_response_map[context_key]
            if responses:
                # Ø§Ø®ØªÙŠØ§Ø± Ø±Ø¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚
                best_response = max(responses.items(), key=lambda x: x[1])[0]
                return self.personalize_response(best_response, context)
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø³ÙŠØ§Ù‚Ø§Ù‹ Ù…Ù†Ø§Ø³Ø¨Ø§Ù‹ØŒ Ù†ÙˆÙ„Ø¯ Ø±Ø¯ Ø¹Ø§Ù…
        return self.generate_general_contextual_response(context)

    def personalize_response(self, base_response: str, context: ConversationContext) -> str:
        """ØªØ®ØµÙŠØµ Ø§Ù„Ø±Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ø´Ø®ØµÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø©"""
        
        # ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø­Ø³Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø©
        if context.relationship_level > 0.8:  # Ø¹Ù„Ø§Ù‚Ø© Ù‚ÙˆÙŠØ©
            # Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ø­Ù…ÙŠÙ…Ø©
            intimate_words = ["Ø­Ø¨ÙŠØ¨ÙŠ", "ÙŠØ§ ØºØ§Ù„ÙŠ", "Ø¹Ø²ÙŠØ²ÙŠ", "ÙŠØ§ Ù‚Ù„Ø¨ÙŠ"]
            if not any(word in base_response for word in intimate_words):
                base_response += f" {random.choice(intimate_words)}"
        
        elif context.relationship_level < 0.3:  # Ø¹Ù„Ø§Ù‚Ø© Ø±Ø³Ù…ÙŠØ©
            # Ø¬Ø¹Ù„ Ø§Ù„Ø±Ø¯ Ø£ÙƒØ«Ø± Ø±Ø³Ù…ÙŠØ©
            formal_endings = ["ØªØ­ÙŠØ§ØªÙŠ", "Ù…Ø¹ Ø§Ø­ØªØ±Ø§Ù…ÙŠ", "Ø¨Ø§Ù„ØªÙˆÙÙŠÙ‚"]
            base_response += f" {random.choice(formal_endings)}"
        
        # ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø´Ø®ØµÙŠØ©
        if context.user_personality_type == "friendly":
            base_response = self.add_friendly_tone(base_response)
        elif context.user_personality_type == "serious":
            base_response = self.add_serious_tone(base_response)
        
        return base_response

    def add_friendly_tone(self, response: str) -> str:
        """Ø¥Ø¶Ø§ÙØ© Ù†Ø¨Ø±Ø© ÙˆØ¯ÙˆØ¯Ø©"""
        friendly_additions = ["ğŸ˜Š", "Ù‡Ù‡Ù‡Ù‡", "ÙˆØ§Ù„Ù„Ù‡", "Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡"]
        return response + f" {random.choice(friendly_additions)}"

    def add_serious_tone(self, response: str) -> str:
        """Ø¥Ø¶Ø§ÙØ© Ù†Ø¨Ø±Ø© Ø¬Ø¯ÙŠØ©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø§Ù…ÙŠØ©
        response = re.sub(r'[ğŸ˜ŠğŸ˜‚ğŸ¤£ğŸ˜]', '', response)
        response = response.replace("Ù‡Ù‡Ù‡Ù‡", "").replace("ÙˆØ§Ù„Ù„Ù‡", "")
        return response.strip()

    def generate_general_contextual_response(self, context: ConversationContext) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø³ÙŠØ§Ù‚ÙŠ Ø¹Ø§Ù…"""
        
        topic_responses = {
            "ØªØ­ÙŠØ©": ["Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ", "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹", "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…"],
            "Ø³Ø¤Ø§Ù„": ["Ø³Ø¤Ø§Ù„ Ù…Ø«ÙŠØ± Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù…", "Ø¯Ø¹Ù†ÙŠ Ø£ÙÙƒØ± ÙÙŠ Ù‡Ø°Ø§", "Ù‡Ø°Ø§ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù‡Ù…"],
            "Ù…Ø´ÙƒÙ„Ø©": ["Ø£ÙÙ‡Ù… Ù…Ø´ÙƒÙ„ØªÙƒ", "Ù‡Ø°Ø§ Ù…Ø­Ø¨Ø· ÙØ¹Ù„Ø§Ù‹", "Ù„Ø§ ØªÙ‚Ù„Ù‚ØŒ Ø³Ù†Ø¬Ø¯ Ø­Ù„Ø§Ù‹"],
            "ÙØ±Ø­": ["Ù‡Ø°Ø§ Ø±Ø§Ø¦Ø¹!", "Ù…Ø¨Ø±ÙˆÙƒ Ø¹Ù„ÙŠÙƒ", "Ø£Ø´Ø§Ø±ÙƒÙƒ Ø§Ù„ÙØ±Ø­Ø©"],
            "Ø¹Ø§Ù…": ["ÙÙ‡Ù…Øª", "Ù†Ø¹Ù…", "Ø·Ø¨Ø¹Ø§Ù‹", "Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯"]
        }
        
        topic = context.conversation_topic if context.conversation_topic in topic_responses else "Ø¹Ø§Ù…"
        return random.choice(topic_responses[topic])

    def add_variation_to_response(self, base_response: str) -> str:
        """Ø¥Ø¶Ø§ÙØ© ØªÙ†ÙˆÙŠØ¹ Ù„Ù„Ø±Ø¯ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
        
        # Ø¥Ø¶Ø§ÙØ§Øª Ø¨Ø³ÙŠØ·Ø© Ù„Ù„ØªÙ†ÙˆÙŠØ¹
        variations = {
            "prefixes": ["", "ÙŠØ¹Ù†ÙŠ", "Ø§Ù„ØµØ±Ø§Ø­Ø©", "Ø·Ø¨Ø¹Ø§Ù‹", "Ø£ÙƒÙŠØ¯"],
            "suffixes": ["", "Ù…Ø§ Ø±Ø£ÙŠÙƒØŸ", "ØµØ­ÙŠØ­ØŸ", "ØªÙ…Ø§Ù…ØŸ", "ÙˆØ§Ø¶Ø­ØŸ"],
            "intensifiers": ["Ø¬Ø¯Ø§Ù‹", "ÙƒØ«ÙŠØ±", "ÙØ¹Ù„Ø§Ù‹", "Ø­Ù‚Ø§Ù‹", ""]
        }
        
        # ØªØ·Ø¨ÙŠÙ‚ ØªÙ†ÙˆÙŠØ¹ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¨Ø³ÙŠØ·
        if random.random() < 0.3:  # 30% Ø§Ø­ØªÙ…Ø§Ù„ Ø¥Ø¶Ø§ÙØ© Ø¨Ø¯Ø§ÙŠØ©
            prefix = random.choice(variations["prefixes"])
            if prefix:
                base_response = f"{prefix} {base_response}"
        
        if random.random() < 0.2:  # 20% Ø§Ø­ØªÙ…Ø§Ù„ Ø¥Ø¶Ø§ÙØ© Ù†Ù‡Ø§ÙŠØ©
            suffix = random.choice(variations["suffixes"])
            if suffix:
                base_response = f"{base_response} {suffix}"
        
        return base_response

    def select_best_response(self, candidates: List[Tuple[str, str, float]], 
                           user_input: str) -> Tuple[str, str, float]:
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø±Ø¯ Ù…Ù† Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†"""
        
        if not candidates:
            return self.generate_fallback_response(), "fallback", 0.3
        
        # ØªØµÙÙŠØ© Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„ÙØ§Ø±ØºØ© Ø£Ùˆ ØºÙŠØ± Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
        valid_candidates = []
        for response, method, confidence in candidates:
            if response and len(response.strip()) > 0 and response != user_input:
                quality_score = self.evaluate_response_quality(response, user_input)
                adjusted_confidence = confidence * quality_score
                valid_candidates.append((response, method, adjusted_confidence))
        
        if not valid_candidates:
            return self.generate_fallback_response(), "fallback", 0.3
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø±Ø¯
        best_candidate = max(valid_candidates, key=lambda x: x[2])
        return best_candidate

    def evaluate_response_quality(self, response: str, user_input: str) -> float:
        """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø±Ø¯"""
        
        quality_score = 1.0
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        if len(response) < 3:
            quality_score -= 0.5
        elif len(response) > 200:
            quality_score -= 0.2
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ù… Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
        if response.lower() == user_input.lower():
            quality_score -= 0.8
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø§Øª Ù…ÙÙŠØ¯Ø©
        meaningful_words = len([w for w in response.split() if len(w) > 2])
        if meaningful_words < 2:
            quality_score -= 0.3
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªÙ†ÙˆØ¹ Ø§Ù„Ù„ØºÙˆÙŠ
        unique_words = len(set(response.lower().split()))
        total_words = len(response.split())
        diversity = unique_words / total_words if total_words > 0 else 0
        quality_score += diversity * 0.2
        
        return max(0.1, quality_score)

    def generate_fallback_response(self) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¹Ù†Ø¯ ÙØ´Ù„ ÙƒÙ„ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø£Ø®Ø±Ù‰"""
        
        fallbacks = [
            "ÙÙ‡Ù…Øª ÙƒÙ„Ø§Ù…Ùƒ",
            "Ø£Ù‡Ø§ØŒ ÙˆØ§Ø¶Ø­",
            "Ø·ÙŠØ¨ØŒ ØªÙ…Ø§Ù…",
            "Ø¥ÙŠÙ‡ Ù†Ø¹Ù…",
            "ØµØ­ÙŠØ­",
            "Ø£ÙƒÙŠØ¯",
            "Ù…Ø§ Ø±Ø£ÙŠÙƒ ÙÙŠ Ù…ÙˆØ¶ÙˆØ¹ Ø¢Ø®Ø±ØŸ",
            "Ø­Ù„Ùˆ ÙƒÙ„Ø§Ù…Ùƒ",
            "Ø²ÙŠÙ†"
        ]
        
        return random.choice(fallbacks)

    def learn_from_feedback(self, user_input: str, bot_response: str, 
                           feedback_type: str, success_score: float):
        """Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
        
        if success_score > 0.6:  # Ø±Ø¯ Ù†Ø§Ø¬Ø­
            self.add_successful_pattern(
                input_sample=user_input,
                response=bot_response,
                context_type=feedback_type,
                emotion="Ù…Ø­Ø§ÙŠØ¯",  # Ø³ÙŠØªÙ… ØªØ­Ø³ÙŠÙ† Ù‡Ø°Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹
                success_score=success_score
            )
        else:  # Ø±Ø¯ ØºÙŠØ± Ù†Ø§Ø¬Ø­ - ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ÙˆØ²Ù†
            self.reduce_pattern_weight(user_input, bot_response, success_score)

    def reduce_pattern_weight(self, user_input: str, bot_response: str, penalty_score: float):
        """ØªÙ‚Ù„ÙŠÙ„ ÙˆØ²Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø· ØºÙŠØ± Ø§Ù„Ù†Ø§Ø¬Ø­Ø©"""
        
        input_keywords = self.extract_keywords(user_input)
        response_words = bot_response.split()
        
        # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªØ±Ø§Ø¨Ø·Ø§Øª
        for input_word in input_keywords:
            for response_word in response_words:
                if input_word in self.word_associations and response_word in self.word_associations[input_word]:
                    self.word_associations[input_word][response_word] *= (1 - penalty_score * 0.1)
                    
                    # Ø­Ø°Ù Ø§Ù„ØªØ±Ø§Ø¨Ø·Ø§Øª Ø§Ù„Ø¶Ø¹ÙŠÙØ© Ø¬Ø¯Ø§Ù‹
                    if self.word_associations[input_word][response_word] < 0.01:
                        del self.word_associations[input_word][response_word]

    def save_learned_data(self):
        """Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©"""
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        os.makedirs(self.data_path, exist_ok=True)
        
        # Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ù…Ù„ÙØ§Øª Ù…Ù†ÙØµÙ„Ø©
        data_files = {
            'word_associations.pkl': dict(self.word_associations),
            'context_response_map.pkl': dict(self.context_response_map),
            'response_patterns.pkl': [
                {
                    'input_pattern': p.input_pattern,
                    'response_template': p.response_template,
                    'success_rate': p.success_rate,
                    'context_type': p.context_type,
                    'emotion_trigger': p.emotion_trigger,
                    'usage_count': p.usage_count,
                    'last_used': p.last_used
                } for p in self.response_patterns
            ],
            'learning_metrics.json': self.learning_metrics,
            'successful_interactions.pkl': list(self.successful_interactions)
        }
        
        for filename, data in data_files.items():
            filepath = os.path.join(self.data_path, filename)
            
            if filename.endswith('.json'):
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            else:
                with open(filepath, 'wb') as f:
                    pickle.dump(data, f)

    def load_learned_data(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©"""
        
        try:
            # ØªØ­Ù…ÙŠÙ„ ØªØ±Ø§Ø¨Ø·Ø§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª
            word_assoc_path = os.path.join(self.data_path, 'word_associations.pkl')
            if os.path.exists(word_assoc_path):
                with open(word_assoc_path, 'rb') as f:
                    data = pickle.load(f)
                    self.word_associations = defaultdict(lambda: defaultdict(float), data)
            
            # ØªØ­Ù…ÙŠÙ„ Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø³ÙŠØ§Ù‚
            context_path = os.path.join(self.data_path, 'context_response_map.pkl')
            if os.path.exists(context_path):
                with open(context_path, 'rb') as f:
                    data = pickle.load(f)
                    self.context_response_map = defaultdict(lambda: defaultdict(float), data)
            
            # ØªØ­Ù…ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø±Ø¯ÙˆØ¯
            patterns_path = os.path.join(self.data_path, 'response_patterns.pkl')
            if os.path.exists(patterns_path):
                with open(patterns_path, 'rb') as f:
                    patterns_data = pickle.load(f)
                    self.response_patterns = [
                        ResponsePattern(**pattern_data) for pattern_data in patterns_data
                    ]
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
            metrics_path = os.path.join(self.data_path, 'learning_metrics.json')
            if os.path.exists(metrics_path):
                with open(metrics_path, 'r', encoding='utf-8') as f:
                    self.learning_metrics = json.load(f)
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©
            interactions_path = os.path.join(self.data_path, 'successful_interactions.pkl')
            if os.path.exists(interactions_path):
                with open(interactions_path, 'rb') as f:
                    interactions_data = pickle.load(f)
                    self.successful_interactions = deque(interactions_data[-500:], maxlen=500)
                    
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©: {e}")
            # Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©

    def get_learning_statistics(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù…"""
        
        stats = {
            "total_patterns": len(self.response_patterns),
            "total_interactions": len(self.successful_interactions),
            "unique_word_associations": len(self.word_associations),
            "context_mappings": len(self.context_response_map),
            "average_success_rate": 0.0,
            "most_successful_patterns": [],
            "learning_progress": self.calculate_learning_progress()
        }
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­
        if self.response_patterns:
            total_success = sum(p.success_rate for p in self.response_patterns)
            stats["average_success_rate"] = total_success / len(self.response_patterns)
        
        # Ø£ÙØ¶Ù„ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
        top_patterns = sorted(self.response_patterns, 
                            key=lambda p: p.success_rate * p.usage_count, 
                            reverse=True)[:5]
        
        stats["most_successful_patterns"] = [
            {
                "pattern": p.input_pattern[:3],  # Ø£ÙˆÙ„ 3 ÙƒÙ„Ù…Ø§Øª
                "response": p.response_template[:50],  # Ø£ÙˆÙ„ 50 Ø­Ø±Ù
                "success_rate": p.success_rate,
                "usage_count": p.usage_count
            } for p in top_patterns
        ]
        
        return stats

    def calculate_learning_progress(self) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø¯Ù‰ ØªÙ‚Ø¯Ù… Ø§Ù„ØªØ¹Ù„Ù…"""
        
        if len(self.successful_interactions) < 10:
            return 0.0
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ Ø£ÙˆÙ„ ÙˆØ¢Ø®Ø± Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª
        interactions_list = list(self.successful_interactions)
        
        first_batch = interactions_list[:10]
        last_batch = interactions_list[-10:]
        
        first_avg = sum(i['success_score'] for i in first_batch) / 10
        last_avg = sum(i['success_score'] for i in last_batch) / 10
        
        return max(0.0, last_avg - first_avg)  # Ø§Ù„ØªØ­Ø³Ù† ÙÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡