# smart_auto_trainer.py - Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù†Ø§Ù†Ùˆ
import json
import re
import requests
import time
import schedule
import logging
from datetime import datetime, timedelta
from pathlib import Path
import random
import hashlib
from typing import List, Dict, Set
# import tweepy  # Ù„ÙŠØ³ Ù…Ø·Ù„ÙˆØ¨ ÙÙŠ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©
# from textblob import TextBlob  # Ù„ÙŠØ³ Ù…Ø·Ù„ÙˆØ¨ Ø­Ø§Ù„ÙŠØ§Ù‹ 
# import nltk  # Ù„ÙŠØ³ Ù…Ø·Ù„ÙˆØ¨ Ø­Ø§Ù„ÙŠØ§Ù‹
from collections import Counter
import threading
import sqlite3

# ØªØ­Ù…ÙŠÙ„ Ù…ÙƒØªØ¨Ø§Øª NLTK Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© - Ù…Ø¹Ø·Ù„ Ø­Ø§Ù„ÙŠØ§Ù‹
# try:
#     nltk.download('punkt', quiet=True)
#     nltk.download('stopwords', quiet=True)
# except:
#     pass

class SmartAutoTrainer:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ - ÙŠØ¬Ù…Ø¹ ÙˆÙŠØ­Ù„Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©"""
    
    def __init__(self):
        self.setup_logging()
        self.setup_database()
        self.load_config()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ ÙÙ„Ø§ØªØ± Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
        self.saudi_indicators = {
            'ÙƒÙ„Ù…Ø§Øª_Ø¯Ù„Ø§Ù„ÙŠØ©': [
                'ÙˆØ´', 'Ø´Ù„ÙˆÙ†', 'ÙƒÙŠÙÙƒ', 'Ù‡Ù„Ø§', 'Ø£Ù‡Ù„ÙŠÙ†', 'Ù…Ø´ÙƒÙˆØ±', 'ÙŠØ¹Ø·ÙŠÙƒ', 'Ø§Ù„Ø¹Ø§ÙÙŠØ©',
                'Ø¨Ø±ÙˆØ­', 'Ø£Ø±Ø¬Ø¹', 'ØªØ±Ø§Ù†ÙŠ', 'Ø®Ù„Ø§Øµ', 'Ø·ÙŠØ¨', 'Ø²ÙŠÙ†', 'ÙƒÙÙˆ', 'ÙŠØ§Ù„Ù„Ù‡',
                'Ø§Ù„Ø­ÙŠÙ†', 'Ø´ÙˆÙŠ', 'Ù…Ø±Ù‡', 'Ø¨Ø·Ù„', 'Ø¹Ø§Ø¯', 'Ù„Ø§ Ø¨Ø£Ø³', 'Ù…Ø§ Ø¹Ù„ÙŠÙ†Ø§',
                'Ø§Ø³ØªØºÙØ±', 'Ø§Ù„Ù„Ù‡', 'ÙŠØ±Ø­Ù…', 'ÙˆØ§Ù„Ø¯ÙŠÙƒ', 'Ø§Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡', 'Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡'
            ],
            'Ù†Ù‡Ø§ÙŠØ§Øª_ÙƒÙ„Ù…Ø§Øª': ['ÙŠÙ†', 'ÙˆÙ‡', 'Ø§Ùƒ', 'ÙƒÙ…', 'Ù‡Ù…', 'Ù†Ø§'],
            'Ø¨Ø¯Ø§ÙŠØ§Øª_ÙƒÙ„Ù…Ø§Øª': ['Ø¨Ø§', 'ÙŠØ§', 'Ù…Ø§', 'Ù„Ø§', 'ÙƒØ§'],
            'ØªØ¹Ø¨ÙŠØ±Ø§Øª_Ù…Ù…ÙŠØ²Ø©': [
                'Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©', 'Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡', 'Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡', 'Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡',
                'ÙŠØ§ Ù‡Ù„Ø§', 'Ø£Ù‡Ù„Ø§ ÙˆØ³Ù‡Ù„Ø§', 'ÙƒÙŠÙ Ø§Ù„ØµØ­Ø©', 'Ø´Ù„ÙˆÙ†Ùƒ', 'ÙˆØ´ Ø§Ø®Ø¨Ø§Ø±Ùƒ'
            ]
        }
        
        # Ù†Ø¸Ø§Ù… ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØªØ·ÙˆØ±
        self.emotion_patterns = {
            'ÙØ±Ø­': ['ÙØ±Ø­Ø§Ù†', 'Ù…Ø¨Ø³ÙˆØ·', 'Ø³Ø¹ÙŠØ¯', 'ÙØ±Ø­Ø©', 'Ø¨Ù‡Ø¬Ø©', 'ğŸ˜Š', 'ğŸ˜„', 'ğŸ˜‚', 'â¤ï¸'],
            'Ø­Ø²Ù†': ['Ø²Ø¹Ù„Ø§Ù†', 'Ø­Ø²ÙŠÙ†', 'Ù…ÙƒØªØ¦Ø¨', 'ØªØ¹Ø¨Ø§Ù†', 'ğŸ˜¢', 'ğŸ˜­', 'ğŸ’”'],
            'ØºØ¶Ø¨': ['Ø²Ø¹Ù„Ø§Ù†', 'Ù…ØªØ¶Ø§ÙŠÙ‚', 'Ù…Ù‚Ù‡ÙˆØ±', 'ØºØ§Ø¶Ø¨', 'ğŸ˜ ', 'ğŸ˜¡'],
            'Ø¯Ø¹Ø§Ø¡': ['Ø§Ù„Ù„Ù‡', 'ÙŠØ±Ø­Ù…', 'ÙŠØ´ÙÙŠ', 'ÙŠØ­ÙØ¸', 'Ø§Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡', 'ÙŠØ§ Ø±Ø¨'],
            'ØªØ­ÙŠØ©': ['Ø§Ù„Ø³Ù„Ø§Ù…', 'ØµØ¨Ø§Ø­', 'Ù…Ø³Ø§Ø¡', 'Ø£Ù‡Ù„Ø§', 'Ù‡Ù„Ø§', 'Ù…Ø±Ø­Ø¨Ø§'],
            'Ø´ÙƒØ±': ['Ø´ÙƒØ±Ø§', 'Ù…Ø´ÙƒÙˆØ±', 'ØªØ³Ù„Ù…', 'ÙŠØ¹Ø·ÙŠÙƒ', 'Ø§Ù„Ø¹Ø§ÙÙŠØ©', 'Ù…Ø§ Ù‚ØµØ±Øª'],
            'Ø§Ø³ØªÙÙ‡Ø§Ù…': ['ÙˆØ´', 'Ù…ØªÙ‰', 'ÙˆÙŠÙ†', 'ÙƒÙŠÙ', 'Ù„ÙŠØ´', 'Ø´Ù„ÙˆÙ†'],
            'ØªØ£ÙƒÙŠØ¯': ['Ø§ÙŠÙ‡', 'ØµØ­', 'Ø¨Ø§Ù„Ø¶Ø¨Ø·', 'Ø²ÙŠÙ†', 'ØªÙ…Ø§Ù…', 'ÙƒØ°Ø§']
        }
    
    def setup_logging(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„"""
        logging.basicConfig(
            filename=f'smart_training_{datetime.now().strftime("%Y%m%d")}.log',
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            encoding='utf-8'
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_database(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ø¤Ù‚Øª ÙˆØ§Ù„ØªØªØ¨Ø¹"""
        self.db_path = "smart_training_cache.db"
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS collected_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source TEXT,
                content TEXT,
                emotion_score TEXT,
                quality_score REAL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                used_in_training BOOLEAN DEFAULT FALSE,
                content_hash TEXT UNIQUE
            )
        ''')
        
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS training_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_date DATETIME,
                sentences_added INTEGER,
                source_breakdown TEXT,
                performance_metrics TEXT
            )
        ''')
        
        self.conn.commit()
    
    def load_config(self):
        """ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        self.config = {
            'training_interval_hours': 2,  # ÙƒÙ„ Ø³Ø§Ø¹ØªÙŠÙ†
            'min_quality_score': 0.3,  # Ø¹ØªØ¨Ø© Ø£Ù‚Ù„ Ù„Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø£ÙƒØ«Ø±
            'max_daily_sentences': 500,
            'sources_enabled': {
                'twitter': True,
                'web_scraping': True,
                'user_conversations': True
            }
        }
    
    def calculate_saudi_score(self, text: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø¯Ù‰ Ø§Ù†ØªÙ…Ø§Ø¡ Ø§Ù„Ù†Øµ Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©"""
        text = text.lower()
        score = 0.0
        total_checks = 0
        
        # ÙØ­Øµ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©
        for word in self.saudi_indicators['ÙƒÙ„Ù…Ø§Øª_Ø¯Ù„Ø§Ù„ÙŠØ©']:
            if word in text:
                score += 0.15
            total_checks += 0.15
        
        # ÙØ­Øµ Ø§Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø©
        for expr in self.saudi_indicators['ØªØ¹Ø¨ÙŠØ±Ø§Øª_Ù…Ù…ÙŠØ²Ø©']:
            if expr in text:
                score += 0.25
            total_checks += 0.25
        
        # ÙØ­Øµ Ø§Ù„Ù†Ù‡Ø§ÙŠØ§Øª ÙˆØ§Ù„Ø¨Ø¯Ø§ÙŠØ§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø©
        words = text.split()
        for word in words:
            for ending in self.saudi_indicators['Ù†Ù‡Ø§ÙŠØ§Øª_ÙƒÙ„Ù…Ø§Øª']:
                if word.endswith(ending) and len(word) > 3:
                    score += 0.05
                    break
            total_checks += 0.05
            
            for beginning in self.saudi_indicators['Ø¨Ø¯Ø§ÙŠØ§Øª_ÙƒÙ„Ù…Ø§Øª']:
                if word.startswith(beginning) and len(word) > 3:
                    score += 0.05
                    break
            total_checks += 0.05
        
        return min(score / max(total_checks, 1), 1.0) if total_checks > 0 else 0.0
    
    def analyze_emotion_context(self, text: str) -> Dict[str, float]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚"""
        text = text.lower()
        emotions = {}
        
        for emotion, patterns in self.emotion_patterns.items():
            score = 0.0
            for pattern in patterns:
                if pattern in text:
                    # ÙˆØ²Ù† Ø£ÙƒØ¨Ø± Ù„Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
                    weight = len(pattern.split()) * 0.3
                    score += weight
            emotions[emotion] = min(score, 1.0)
        
        return emotions
    
    def quality_check(self, text: str) -> float:
        """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Øµ Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        if len(text) < 5 or len(text) > 300:  # Ù…Ø¯Ù‰ Ø£ÙˆØ³Ø¹
            return 0.1
        
        # ÙØ­Øµ Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
        saudi_score = self.calculate_saudi_score(text)
        
        # ÙØ­Øµ ØªÙ†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        words = text.split()
        unique_words = len(set(words))
        diversity_score = min(unique_words / max(len(words), 1), 1.0) if words else 0.5
        
        # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø±Ù…ÙˆØ² ØºÙŠØ± Ù…Ø±ØºÙˆØ¨Ø©
        spam_indicators = ['http', 'www', '@', '#hashtag', 'follow', 'like']
        has_spam = any(indicator in text.lower() for indicator in spam_indicators)
        
        # ØªÙ‚ÙŠÙŠÙ… Ù…Ø¨Ø³Ø· ÙˆÙˆØ§Ù‚Ø¹ÙŠ
        if has_spam:
            final_score = 0.1  # Ø¬ÙˆØ¯Ø© Ù…Ù†Ø®ÙØ¶Ø© Ù„Ù„spam
        else:
            # Ø¬ÙˆØ¯Ø© Ø£Ø³Ø§Ø³ÙŠØ© + Ù…ÙƒØ§ÙØ¢Øª Ø§Ù„Ù„Ù‡Ø¬Ø© ÙˆØ§Ù„ØªÙ†ÙˆØ¹
            base_score = 0.4  # Ø¬ÙˆØ¯Ø© Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù‚Ø¨ÙˆÙ„Ø©
            saudi_bonus = saudi_score * 0.3  # Ù…ÙƒØ§ÙØ£Ø© Ø§Ù„Ù„Ù‡Ø¬Ø©
            diversity_bonus = diversity_score * 0.2  # Ù…ÙƒØ§ÙØ£Ø© Ø§Ù„ØªÙ†ÙˆØ¹
            final_score = base_score + saudi_bonus + diversity_bonus
        
        return max(0.1, min(final_score, 1.0))
    
    def collect_from_social_media(self) -> List[Dict]:
        """Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ"""
        collected_data = []
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† ØªÙˆÙŠØªØ± (ÙŠØ­ØªØ§Ø¬ API keys Ø­Ù‚ÙŠÙ‚ÙŠØ©)
        sample_tweets = [
            "ÙˆØ´ Ø±Ø§ÙŠÙƒÙ… Ø¨Ø§Ù„Ø·Ù‚Ø³ Ø§Ù„ÙŠÙˆÙ…ØŸ Ø­Ø§Ø± Ù…Ø±Ù‡ ÙˆØ§Ù„Ù„Ù‡",
            "Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø¹Ù„Ù‰ ÙƒÙ„ Ø­Ø§Ù„ØŒ Ø§Ù„ÙŠÙˆÙ… ÙƒØ§Ù† ÙŠÙˆÙ… Ø­Ù„Ùˆ",
            "ÙŠØ§ Ù‡Ù„Ø§ ÙÙŠÙƒ Ø£Ø®ÙˆÙŠØŒ ÙƒÙŠÙ Ø§Ù„ØµØ­Ø© ÙˆØ§Ù„Ø¹Ø§Ø¦Ù„Ø©ØŸ",
            "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ÙˆØ§ÙÙŠ",
            "Ø´ÙƒØ±Ø§ Ù„Ùƒ ÙŠØ§ ØºØ§Ù„ÙŠØŒ Ù…Ø§ Ù‚ØµØ±Øª ÙˆØ§Ù„Ù„Ù‡",
            "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ± Ø¬Ù…ÙŠØ¹Ø§Ù‹ØŒ Ø§Ù„Ù„Ù‡ ÙŠØ¬Ø¹Ù„Ù‡ ÙŠÙˆÙ… Ù…Ø¨Ø§Ø±Ùƒ",
            "ÙˆØ´ Ø§Ø®Ø¨Ø§Ø± Ø§Ù„Ø´ØºÙ„ØŸ Ø§Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ ÙƒÙ„Ù‡ ØªÙ…Ø§Ù…",
            "ÙŠØ§Ù„Ù„Ù‡ Ù†Ø±ÙˆØ­ Ù†ØªØºØ¯Ù‰ØŒ Ø§Ù†Ø§ Ø¬ÙˆØ¹Ø§Ù† Ù…Ø±Ù‡",
            "Ø§Ù„Ù„Ù‡ ÙŠØ´ÙÙŠÙ‡ ÙˆÙŠØ¹Ø§ÙÙŠÙ‡ØŒ Ù†Ø³Ø£Ù„ Ø§Ù„Ù„Ù‡ Ù„Ù‡ Ø§Ù„ØµØ­Ø©",
            "ÙƒÙÙˆ Ø¹Ù„ÙŠÙƒØŒ Ø´ØºÙ„ Ù…ØªÙ‚Ù† Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡"
        ]
        
        for tweet in sample_tweets:
            quality = self.quality_check(tweet)
            if quality >= self.config['min_quality_score']:
                emotions = self.analyze_emotion_context(tweet)
                
                collected_data.append({
                    'source': 'twitter',
                    'content': tweet,
                    'quality_score': quality,
                    'emotions': emotions,
                    'timestamp': datetime.now()
                })
        
        return collected_data
    
    def collect_from_web(self) -> List[Dict]:
        """Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ù…ÙˆØ§Ù‚Ø¹ ÙˆÙŠØ¨ Ù…Ø®ØªÙ„ÙØ©"""
        # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© ÙƒÙˆØ¯ Ù„Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ù…ÙˆØ§Ù‚Ø¹ Ù…Ø®ØªÙ„ÙØ©
        # Ù…Ø«Ù„ Ø§Ù„Ù…Ù†ØªØ¯ÙŠØ§Øª Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŒ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠØ©ØŒ Ø¥Ù„Ø®
        
        sample_web_content = [
            "Ø£Ù‡Ù„Ø§ ÙˆØ³Ù‡Ù„Ø§ Ø¨ÙƒÙ… ÙÙŠ Ù…Ù†ØªØ¯Ø§Ù†Ø§ Ø§Ù„ÙƒØ±ÙŠÙ…",
            "Ù†Ø±Ø­Ø¨ Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø¯ ÙÙŠ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹",
            "Ø´ÙƒØ±Ø§Ù‹ Ù„ÙƒÙ… Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",
            "Ù†ØªØ·Ù„Ø¹ Ù„Ù…Ø´Ø§Ø±ÙƒØ§ØªÙƒÙ… Ø§Ù„Ù‚ÙŠÙ…Ø© ÙˆØ§Ù„Ù…ÙÙŠØ¯Ø©",
            "Ø§Ù„Ù„Ù‡ ÙŠÙˆÙÙ‚ Ø§Ù„Ø¬Ù…ÙŠØ¹ ÙÙŠ Ù…Ø³Ø§Ø¹ÙŠÙ‡Ù…",
        ]
        
        collected_data = []
        for content in sample_web_content:
            quality = self.quality_check(content)
            if quality >= self.config['min_quality_score']:
                emotions = self.analyze_emotion_context(content)
                
                collected_data.append({
                    'source': 'web',
                    'content': content,
                    'quality_score': quality,
                    'emotions': emotions,
                    'timestamp': datetime.now()
                })
        
        return collected_data
    
    def store_collected_data(self, data_list: List[Dict]):
        """Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        for item in data_list:
            content_hash = hashlib.md5(item['content'].encode()).hexdigest()
            
            try:
                self.conn.execute('''
                    INSERT INTO collected_data 
                    (source, content, emotion_score, quality_score, content_hash)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    item['source'],
                    item['content'],
                    json.dumps(item['emotions'], ensure_ascii=False),
                    item['quality_score'],
                    content_hash
                ))
                self.conn.commit()
            except sqlite3.IntegrityError:
                # Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ø§Ù‹
                pass
    
    def get_training_data(self) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        cursor = self.conn.execute('''
            SELECT content FROM collected_data 
            WHERE used_in_training = FALSE AND quality_score >= ?
            ORDER BY quality_score DESC, timestamp DESC
            LIMIT ?
        ''', (self.config['min_quality_score'], self.config['max_daily_sentences']))
        
        sentences = [row[0] for row in cursor.fetchall()]
        
        # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
        if sentences:
            placeholders = ','.join('?' * len(sentences))
            self.conn.execute(f'''
                UPDATE collected_data 
                SET used_in_training = TRUE 
                WHERE content IN ({placeholders})
            ''', sentences)
            self.conn.commit()
        
        return sentences
    
    def update_nano_corpus(self, new_sentences: List[str]):
        """ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ø§Ù†Ùˆ Ø¨Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
        corpus_path = "corpus.json"
        
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø­Ø§Ù„ÙŠ
            if Path(corpus_path).exists():
                with open(corpus_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                data = {"sentences": []}
            
            current_sentences = set(data.get("sentences", []))
            added_count = 0
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            for sentence in new_sentences:
                if sentence not in current_sentences:
                    data["sentences"].append(sentence)
                    current_sentences.add(sentence)
                    added_count += 1
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø¯Ø«
            with open(corpus_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ØªÙ… Ø¥Ø¶Ø§ÙØ© {added_count} Ø¬Ù…Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            print(f"âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© {added_count} Ø¬Ù…Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©")
            print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¬Ù…Ù„: {len(data['sentences'])}")
            
            return added_count
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
            print(f"âŒ Ø®Ø·Ø£: {str(e)}")
            return 0
    
    def train_nano_system(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ù†Ø¸Ø§Ù… Ù†Ø§Ù†Ùˆ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
        try:
            # Ù‡Ù†Ø§ ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            from core.nano_brain import NanoBrain
            
            nano_brain = NanoBrain()
            nano_brain.retrain_with_new_data()
            
            print("ğŸ§  ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ù†Ø§Ù†Ùˆ Ø¨Ù†Ø¬Ø§Ø­")
            self.logger.info("ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ù†Ø§Ù†Ùˆ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {str(e)}")
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ù†Ø§Ù†Ùˆ: {str(e)}")
    
    def run_training_cycle(self):
        """ØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© ØªØ¯Ø±ÙŠØ¨ ÙƒØ§Ù…Ù„Ø©"""
        start_time = datetime.now()
        print(f"\nğŸš€ Ø¨Ø¯Ø¡ Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ - {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        try:
            # Ù…Ø±Ø­Ù„Ø© 1: Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            print("ğŸ“¥ Ù…Ø±Ø­Ù„Ø© 1: Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
            all_data = []
            
            if self.config['sources_enabled']['twitter']:
                social_data = self.collect_from_social_media()
                all_data.extend(social_data)
                print(f"   ØªÙˆÙŠØªØ±: {len(social_data)} Ø¹Ù†ØµØ±")
            
            if self.config['sources_enabled']['web_scraping']:
                web_data = self.collect_from_web()
                all_data.extend(web_data)
                print(f"   Ø§Ù„ÙˆÙŠØ¨: {len(web_data)} Ø¹Ù†ØµØ±")
            
            # Ù…Ø±Ø­Ù„Ø© 2: Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            print("ğŸ’¾ Ù…Ø±Ø­Ù„Ø© 2: Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
            self.store_collected_data(all_data)
            
            # Ù…Ø±Ø­Ù„Ø© 3: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            print("ğŸ” Ù…Ø±Ø­Ù„Ø© 3: Ø§Ø®ØªÙŠØ§Ø± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
            training_sentences = self.get_training_data()
            print(f"   ØªÙ… Ø§Ø®ØªÙŠØ§Ø± {len(training_sentences)} Ø¬Ù…Ù„Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨")
            
            # Ù…Ø±Ø­Ù„Ø© 4: ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            print("ğŸ“ Ù…Ø±Ø­Ù„Ø© 4: ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
            added_count = self.update_nano_corpus(training_sentences)
            
            # Ù…Ø±Ø­Ù„Ø© 5: Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            if added_count > 0:
                print("ğŸ§  Ù…Ø±Ø­Ù„Ø© 5: Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ù†Ø§Ù†Ùˆ...")
                self.train_nano_system()
            else:
                print("â­ï¸ ØªÙ… ØªØ®Ø·ÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©")
            
            # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø©
            end_time = datetime.now()
            duration = end_time - start_time
            
            self.conn.execute('''
                INSERT INTO training_sessions 
                (session_date, sentences_added, source_breakdown, performance_metrics)
                VALUES (?, ?, ?, ?)
            ''', (
                start_time,
                added_count,
                json.dumps({src: len([d for d in all_data if d['source'] == src]) 
                          for src in ['twitter', 'web']}, ensure_ascii=False),
                json.dumps({'duration_minutes': duration.total_seconds() / 60}, ensure_ascii=False)
            ))
            self.conn.commit()
            
            print("=" * 60)
            print(f"âœ… Ø§ÙƒØªÙ…Ù„Øª Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
            print(f"â±ï¸ Ø§Ù„Ù…Ø¯Ø©: {duration.total_seconds():.1f} Ø«Ø§Ù†ÙŠØ©")
            print(f"ğŸ“ˆ Ø¬Ù…Ù„ Ø¬Ø¯ÙŠØ¯Ø©: {added_count}")
            
            self.logger.info(f"Ø¯ÙˆØ±Ø© ØªØ¯Ø±ÙŠØ¨ Ù†Ø§Ø¬Ø­Ø©: {added_count} Ø¬Ù…Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ {duration.total_seconds():.1f} Ø«Ø§Ù†ÙŠØ©")
            
        except Exception as e:
            error_msg = f"Ø®Ø·Ø£ ÙÙŠ Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            self.logger.error(error_msg)
    
    def start_continuous_training(self):
        """Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
        print("ğŸ”„ Ø¨Ø¯Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø§Ù„Ù…Ø³ØªÙ…Ø±")
        print(f"â° Ø³ÙŠØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙƒÙ„ {self.config['training_interval_hours']} Ø³Ø§Ø¹Ø©")
        print("ğŸ›‘ Ø§Ø¶ØºØ· Ctrl+C Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù†Ø¸Ø§Ù…")
        print("-" * 50)
        
        # Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        schedule.every(self.config['training_interval_hours']).hours.do(self.run_training_cycle)
        
        # ØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© ÙÙˆØ±ÙŠØ©
        print("â–¶ï¸ ØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© ØªØ¯Ø±ÙŠØ¨ ÙÙˆØ±ÙŠØ©...")
        self.run_training_cycle()
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # ÙØ­Øµ ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ")
            self.logger.info("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
        finally:
            self.conn.close()
    
    def get_statistics(self):
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
        cursor = self.conn.execute('''
            SELECT 
                COUNT(*) as total_collected,
                AVG(quality_score) as avg_quality,
                SUM(CASE WHEN used_in_training = TRUE THEN 1 ELSE 0 END) as used_count,
                COUNT(DISTINCT source) as unique_sources
            FROM collected_data
        ''')
        
        stats = cursor.fetchone()
        
        print("\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø°ÙƒÙŠ")
        print("=" * 40)
        print(f"ğŸ”¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©: {stats[0]}")
        print(f"â­ Ù…ØªÙˆØ³Ø· Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {stats[1]:.2f}")
        print(f"ğŸ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {stats[2]}")
        print(f"ğŸ“¡ Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø±: {stats[3]}")

if __name__ == "__main__":
    trainer = SmartAutoTrainer()
    
    print("ğŸ¤– Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù†Ø§Ù†Ùˆ")
    print("=" * 50)
    print("1. Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ…Ø±")
    print("2. ØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø­Ø¯Ø©")
    print("3. Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")
    print("4. Ø§Ø®ØªØ¨Ø§Ø± Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Øµ")
    
    choice = input("\nØ§Ø®ØªØ± Ø±Ù‚Ù… Ø§Ù„Ø¹Ù…Ù„ÙŠØ©: ").strip()
    
    if choice == "1":
        trainer.start_continuous_training()
    elif choice == "2":
        trainer.run_training_cycle()
    elif choice == "3":
        trainer.get_statistics()
    elif choice == "4":
        test_text = input("Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±: ")
        quality = trainer.quality_check(test_text)
        saudi_score = trainer.calculate_saudi_score(test_text)
        emotions = trainer.analyze_emotion_context(test_text)
        
        print(f"\nğŸ“ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
        print(f"ğŸ¯ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Øµ: {quality:.2f}")
        print(f"ğŸ‡¸ğŸ‡¦ Ø¯Ø±Ø¬Ø© Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©: {saudi_score:.2f}")
        print(f"ğŸ’­ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {emotions}")
    else:
        print("âŒ Ø§Ø®ØªÙŠØ§Ø± ØºÙŠØ± ØµØ­ÙŠØ­")