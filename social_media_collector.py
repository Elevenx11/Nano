# social_media_collector.py - Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ
import requests
import json
import re
import time
from datetime import datetime, timedelta
import logging
from typing import List, Dict, Optional
import hashlib
from urllib.parse import quote_plus
import random

class SocialMediaCollector:
    """Ø¬Ø§Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ"""
    
    def __init__(self):
        self.setup_logging()
        
        # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ
        self.saudi_keywords = [
            "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©", "Ø§Ù„Ø±ÙŠØ§Ø¶", "Ø¬Ø¯Ø©", "Ø§Ù„Ø¯Ù…Ø§Ù…", "Ù…ÙƒØ©", "Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©",
            "ÙˆØ´ Ø±Ø§ÙŠÙƒÙ…", "ÙƒÙŠÙÙƒÙ…", "Ø´Ù„ÙˆÙ†ÙƒÙ…", "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒÙ… Ø§Ù„Ø¹Ø§ÙÙŠØ©",
            "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", "Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±", "ÙƒÙ„ Ø¹Ø§Ù… ÙˆØ§Ù†ØªÙ… Ø¨Ø®ÙŠØ±",
            "Ø§Ù„Ù„Ù‡ ÙŠÙˆÙÙ‚Ùƒ", "Ø§Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡", "Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡", "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡",
            "ÙŠØ§Ù„Ù„Ù‡", "Ø·ÙŠØ¨", "Ø²ÙŠÙ†", "ÙƒÙÙˆ", "Ù…Ø´ÙƒÙˆØ±", "ØªØ³Ù„Ù…",
            "Ø§Ù„Ø­ÙŠÙ†", "Ø´ÙˆÙŠ", "Ù…Ø±Ù‡", "Ø¨Ø·Ù„", "Ø¹Ø§Ø¯", "Ø®Ù„Ø§Øµ"
        ]
    
    def setup_logging(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„"""
        logging.basicConfig(
            filename=f'social_collector_{datetime.now().strftime("%Y%m%d")}.log',
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            encoding='utf-8'
        )
        self.logger = logging.getLogger(__name__)
    
    def collect_from_twitter_api(self, keywords: List[str], max_results: int = 100) -> List[Dict]:
        """Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† ØªÙˆÙŠØªØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… API"""
        collected_data = []
        
        # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù…Ù† ØªÙˆÙŠØªØ±
        sample_tweets = [
            {
                'text': "ÙˆØ´ Ø±Ø§ÙŠÙƒÙ… Ø¨Ø§Ù„Ø·Ù‚Ø³ Ø§Ù„ÙŠÙˆÙ…ØŸ Ø­Ø§Ø± Ù…Ø±Ù‡ ÙˆØ§Ù„Ù„Ù‡ â˜€ï¸",
                'created_at': datetime.now().isoformat(),
                'user': {'username': 'user1', 'location': 'Ø§Ù„Ø±ÙŠØ§Ø¶'},
                'public_metrics': {'like_count': 15, 'retweet_count': 3}
            },
            {
                'text': "Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø¹Ù„Ù‰ ÙƒÙ„ Ø­Ø§Ù„ØŒ Ø§Ù„ÙŠÙˆÙ… ÙƒØ§Ù† ÙŠÙˆÙ… Ø­Ù„Ùˆ ğŸ˜Š",
                'created_at': datetime.now().isoformat(),
                'user': {'username': 'user2', 'location': 'Ø¬Ø¯Ø©'},
                'public_metrics': {'like_count': 22, 'retweet_count': 8}
            },
            {
                'text': "ÙŠØ§ Ù‡Ù„Ø§ ÙÙŠÙƒ Ø£Ø®ÙˆÙŠØŒ ÙƒÙŠÙ Ø§Ù„ØµØ­Ø© ÙˆØ§Ù„Ø¹Ø§Ø¦Ù„Ø©ØŸ",
                'created_at': datetime.now().isoformat(),
                'user': {'username': 'user3', 'location': 'Ø§Ù„Ø¯Ù…Ø§Ù…'},
                'public_metrics': {'like_count': 31, 'retweet_count': 5}
            },
            {
                'text': "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ÙˆØ§ÙÙŠ ğŸ‘",
                'created_at': datetime.now().isoformat(),
                'user': {'username': 'user4', 'location': 'Ù…ÙƒØ©'},
                'public_metrics': {'like_count': 45, 'retweet_count': 12}
            },
            {
                'text': "Ø´ÙƒØ±Ø§ Ù„Ùƒ ÙŠØ§ ØºØ§Ù„ÙŠØŒ Ù…Ø§ Ù‚ØµØ±Øª ÙˆØ§Ù„Ù„Ù‡",
                'created_at': datetime.now().isoformat(),
                'user': {'username': 'user5', 'location': 'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©'},
                'public_metrics': {'like_count': 18, 'retweet_count': 4}
            },
            {
                'text': "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ± Ø¹Ù„ÙŠÙƒÙ…ØŒ ÙƒÙŠÙ Ù†ÙˆÙ…ÙƒÙ…ØŸ",
                'created_at': datetime.now().isoformat(),
                'user': {'username': 'user6', 'location': 'Ø§Ù„Ø±ÙŠØ§Ø¶'},
                'public_metrics': {'like_count': 12, 'retweet_count': 2}
            },
            {
                'text': "ÙˆØ§Ù„Ù„Ù‡ Ø§Ù†Ø§ Ù…Ù† Ø¬Ø¯ Ø§Ø³ØªÙØ¯Øª Ù…Ù† Ù‡Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª",
                'created_at': datetime.now().isoformat(),
                'user': {'username': 'user7', 'location': 'Ø¬Ø¯Ø©'},
                'public_metrics': {'like_count': 28, 'retweet_count': 7}
            },
            {
                'text': "Ø§Ù„Ù„Ù‡ ÙŠØ³Ø¹Ø¯Ùƒ ÙˆÙŠÙˆÙÙ‚Ùƒ ÙÙŠ ÙƒÙ„ Ø®Ø·ÙˆØ© ØªØ®Ø·ÙŠÙ‡Ø§",
                'created_at': datetime.now().isoformat(),
                'user': {'username': 'user8', 'location': 'Ø§Ù„Ø·Ø§Ø¦Ù'},
                'public_metrics': {'like_count': 41, 'retweet_count': 11}
            },
            {
                'text': "ÙˆØ´ Ø¨Ø±Ø§Ù…Ø¬ÙƒÙ… Ù„Ù„Ø¹Ø·Ù„Ø© Ù‡Ø§Ù„Ø£ÙŠØ§Ù…ØŸ Ø§Ø¨ØºÙ‰ Ø§Ø±ÙˆØ­ Ù…Ø¹ Ø§Ù„Ø¹ÙŠØ§Ù„",
                'created_at': datetime.now().isoformat(),
                'user': {'username': 'user9', 'location': 'Ø§Ù„Ø±ÙŠØ§Ø¶'},
                'public_metrics': {'like_count': 23, 'retweet_count': 6}
            },
            {
                'text': "Ø§Ù†Ø´Ø§Ù„Ù„Ù‡ Ø§Ù„Ø¬Ùˆ ÙŠØ¹ØªØ¯Ù„ Ù‚Ø±ÙŠØ¨Ø§Ù‹ØŒ Ù…Ø§Ø§Ù‚Ø¯Ø± Ø¹Ù„Ù‰ Ù‡Ø§Ù„Ø­Ø±",
                'created_at': datetime.now().isoformat(),
                'user': {'username': 'user10', 'location': 'Ø¬Ø¯Ø©'},
                'public_metrics': {'like_count': 19, 'retweet_count': 4}
            },
            {
                'text': "ÙŠØ§Ø§Ø§Ø§Ù‡ Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø®Ù„ØµÙ†Ø§ Ù…Ù† Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§Øª Ø¨Ø§Ù„Ø³Ù„Ø§Ù…Ø©",
                'created_at': datetime.now().isoformat(),
                'user': {'username': 'user11', 'location': 'Ø§Ù„Ø¯Ù…Ø§Ù…'},
                'public_metrics': {'like_count': 33, 'retweet_count': 9}
            },
            {
                'text': "Ø§Ù„ÙŠÙˆÙ… Ø¬Ø±Ø¨Øª Ù…Ø·Ø¹Ù… Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø§Ù„Ø­ÙŠØŒ ÙˆØ§Ù„Ù„Ù‡ Ø§ÙƒÙ„Ù‡ Ù„Ø°ÙŠØ°",
                'created_at': datetime.now().isoformat(),
                'user': {'username': 'user12', 'location': 'Ø§Ù„Ø±ÙŠØ§Ø¶'},
                'public_metrics': {'like_count': 27, 'retweet_count': 5}
            }
        ]
        
        for tweet_data in sample_tweets[:max_results]:
            collected_data.append({
                'platform': 'twitter',
                'content': tweet_data['text'],
                'timestamp': tweet_data['created_at'],
                'engagement': tweet_data['public_metrics']['like_count'] + tweet_data['public_metrics']['retweet_count'],
                'location': tweet_data['user'].get('location', ''),
                'source_url': f"https://twitter.com/{tweet_data['user']['username']}"
            })
        
        self.logger.info(f"ØªÙ… Ø¬Ù…Ø¹ {len(collected_data)} ØªØºØ±ÙŠØ¯Ø© Ù…Ù† Twitter")
        return collected_data
    
    def collect_from_reddit(self, subreddits: List[str] = ['saudiarabia', 'riyadh'], max_posts: int = 50) -> List[Dict]:
        """Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Reddit"""
        collected_data = []
        
        # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù…Ù† Reddit
        sample_posts = [
            {
                'title': "Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶ - Ø´Ø§Ø±ÙƒÙˆØ§ ØªØ¬Ø§Ø±Ø¨ÙƒÙ…",
                'selftext': "ÙˆØ´ Ø£ÙØ¶Ù„ Ù…Ø·Ø¹Ù… Ø±Ø­ØªÙˆØ§ Ù„Ù‡ ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ Ø§Ø¨ØºÙ‰ Ø£Ø¬Ø±Ø¨ Ø£Ù…Ø§ÙƒÙ† Ø¬Ø¯ÙŠØ¯Ø©",
                'created_utc': time.time(),
                'score': 25,
                'num_comments': 15,
                'subreddit': 'riyadh'
            },
            {
                'title': "Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø·Ù‚Ø³ Ø§Ù„Ø­Ø§Ø±",
                'selftext': "Ø§Ù„Ø¬Ùˆ Ø­Ø§Ø± Ù…Ø±Ù‡ Ù‡Ø§Ù„Ø£ÙŠØ§Ù…ØŒ ÙˆØ´ Ø£ÙØ¶Ù„ Ø§Ù„Ø·Ø±Ù‚ Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡ØŸ",
                'created_utc': time.time(),
                'score': 42,
                'num_comments': 23,
                'subreddit': 'saudiarabia'
            },
            {
                'title': "ØªØ¬Ø±Ø¨ØªÙŠ Ù…Ø¹ Ø§Ù„Ø´ØºÙ„ ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶",
                'selftext': "Ø­Ø¨ÙŠØª Ø£Ø´Ø§Ø±ÙƒÙƒÙ… ØªØ¬Ø±Ø¨ØªÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ØŒ ÙˆØ§Ù„Ù„Ù‡ ØµØ§Ø± Ù„ÙŠ Ø´Ù‡Ø± ÙˆØ£Ù†Ø§ Ù…Ø¨Ø³ÙˆØ·",
                'created_utc': time.time(),
                'score': 67,
                'num_comments': 31,
                'subreddit': 'riyadh'
            },
            {
                'title': "Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„Ù…Ù…Ù„ÙƒØ©",
                'selftext': "Ø§Ø¨ØºÙ‰ Ø§Ø¹Ø±Ù Ø§ÙŠØ´ Ø£ÙØ¶Ù„ Ø¬Ø§Ù…Ø¹Ø© Ù„ØªØ®ØµØµ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©ØŸ ÙŠØ§Ù„ÙŠØª ØªØ³Ø§Ø¹Ø¯ÙˆÙ†ÙŠ",
                'created_utc': time.time(),
                'score': 34,
                'num_comments': 19,
                'subreddit': 'saudiarabia'
            },
            {
                'title': "ÙƒÙŠÙ Ø§Ù„ÙˆØ¶Ø¹ Ù…Ø¹ ØªÙˆØµÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª",
                'selftext': "ØµØ§Ø± Ù„ÙŠ ÙØªØ±Ø© Ø§Ø³ØªØ®Ø¯Ù… ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙˆØµÙŠÙ„ØŒ Ø¨Ø³ Ø§Ø­Ø³ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± ØºØ§Ù„ÙŠØ© Ø´ÙˆÙŠ",
                'created_utc': time.time(),
                'score': 18,
                'num_comments': 8,
                'subreddit': 'riyadh'
            },
            {
                'title': "ÙˆØ´ Ø£ÙØ¶Ù„ Ù…Ø§Ø±ÙƒØ§Øª ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶",
                'selftext': "Ø§Ø¨ØºÙ‰ Ø§Ø´ØªØ±ÙŠ Ù‡Ø¯Ø§ÙŠØ§ Ù„Ù„Ø¹Ø§Ø¦Ù„Ø©ØŒ ÙˆØ´ ØªÙ†ØµØ­ÙˆÙ†ÙŠ Ù…Ù† Ø§Ù„Ù…Ø§Ø±ÙƒØ§ØªØŸ",
                'created_utc': time.time(),
                'score': 29,
                'num_comments': 16,
                'subreddit': 'riyadh'
            },
            {
                'title': "ØªØ¬Ø±Ø¨ØªÙŠ Ù…Ø¹ Ø·Ù„Ø¨ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù",
                'selftext': "Ø®Ù„ØµØª Ù‚Ø¨Ù„ ÙƒÙ… Ø´Ù‡Ø± ÙˆØ§Ø¨ØºÙ‰ Ø§Ø´ØªØºÙ„ØŒ ÙˆØ´ Ù†ØµØ§Ø¦Ø­ÙƒÙ… Ù„Ù„ØªÙ‚Ø¯ÙŠÙ…ØŸ",
                'created_utc': time.time(),
                'score': 45,
                'num_comments': 22,
                'subreddit': 'saudiarabia'
            }
        ]
        
        for post in sample_posts[:max_posts]:
            if post['selftext']:  # ÙÙ‚Ø· Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Øµ
                collected_data.append({
                    'platform': 'reddit',
                    'content': f"{post['title']} - {post['selftext']}",
                    'timestamp': datetime.fromtimestamp(post['created_utc']).isoformat(),
                    'engagement': post['score'] + post['num_comments'],
                    'location': 'Saudi Arabia',
                    'source_url': f"https://reddit.com/r/{post['subreddit']}"
                })
        
        self.logger.info(f"ØªÙ… Ø¬Ù…Ø¹ {len(collected_data)} Ù…Ù†Ø´ÙˆØ± Ù…Ù† Reddit")
        return collected_data
    
    def collect_from_forums(self) -> List[Dict]:
        """Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù†ØªØ¯ÙŠØ§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©"""
        collected_data = []
        
        # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù†ØªØ¯ÙŠØ§Øª
        sample_forum_posts = [
            {
                'content': "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø±ÙƒØ§ØªÙ‡ØŒ ÙƒÙŠÙÙƒÙ… ÙŠØ§ Ø´Ø¨Ø§Ø¨ØŸ",
                'forum': 'hawamer',
                'timestamp': datetime.now().isoformat(),
                'replies': 12
            },
            {
                'content': "Ø¬Ø±Ø¨Øª Ø§Ù„Ù…Ø·Ø¹Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ÙˆØ§Ù„Ù„Ù‡ ÙƒØ§Ù† Ø£ÙƒÙ„Ù‡ Ø²ÙŠÙ† Ù…Ø±Ù‡",
                'forum': 'saudieng',
                'timestamp': datetime.now().isoformat(),
                'replies': 8
            },
            {
                'content': "ÙˆØ´ Ø±Ø§ÙŠÙƒÙ… ÙÙŠ Ø§Ù„Ø£Ø¬ÙˆØ§Ø¡ Ø§Ù„Ø­Ù„ÙˆØ© Ù‡Ø§Ù„Ø£ÙŠØ§Ù…ØŸ Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø§Ù†ÙƒØ³Ø± Ø§Ù„Ø­Ø± Ø´ÙˆÙŠ",
                'forum': 'almrsal',
                'timestamp': datetime.now().isoformat(),
                'replies': 15
            },
            {
                'content': "Ø§Ø¨ØºÙ‰ Ø§Ø³Ø§ÙØ± Ø®Ø§Ø±Ø¬ Ø§Ù„Ù…Ù…Ù„ÙƒØ©ØŒ ÙˆØ´ ØªÙ†ØµØ­ÙˆÙ†ÙŠ Ù…Ù† Ø§Ù„Ø¯ÙˆÙ„ØŸ",
                'forum': 'travel_ksa',
                'timestamp': datetime.now().isoformat(),
                'replies': 23
            },
            {
                'content': "Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø®Ù„ØµØª Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ø®ÙŠØ±Ø§Ù‹ØŒ Ø§Ø¯Ø¹ÙˆÙ„ÙŠ Ø§Ø´ØªØºÙ„ Ø´ØºÙ„ Ø²ÙŠÙ†",
                'forum': 'graduates',
                'timestamp': datetime.now().isoformat(),
                'replies': 7
            }
        ]
        
        for post in sample_forum_posts:
            collected_data.append({
                'platform': 'forum',
                'content': post['content'],
                'timestamp': post['timestamp'],
                'engagement': post['replies'],
                'location': 'Saudi Arabia',
                'source_url': f"https://www.{post['forum']}.com"
            })
        
        self.logger.info(f"ØªÙ… Ø¬Ù…Ø¹ {len(collected_data)} Ù…Ù†Ø´ÙˆØ± Ù…Ù† Ø§Ù„Ù…Ù†ØªØ¯ÙŠØ§Øª")
        return collected_data
    
    def collect_all_sources(self, max_per_source: int = 100) -> List[Dict]:
        """Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±"""
        all_data = []
        
        print("ğŸ”„ Ø¨Ø¯Ø¡ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±...")
        
        try:
            # Ø¬Ù…Ø¹ Ù…Ù† ØªÙˆÙŠØªØ±
            print("ğŸ“± Ø¬Ù…Ø¹ Ù…Ù† Twitter...")
            twitter_data = self.collect_from_twitter_api(
                keywords=self.saudi_keywords[:5], 
                max_results=min(max_per_source, 8)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©
            )
            all_data.extend(twitter_data)
            
            # Ø¬Ù…Ø¹ Ù…Ù† Reddit
            print("ğŸ”´ Ø¬Ù…Ø¹ Ù…Ù† Reddit...")
            reddit_data = self.collect_from_reddit(max_posts=min(max_per_source, 5))
            all_data.extend(reddit_data)
            
            # Ø¬Ù…Ø¹ Ù…Ù† Ø§Ù„Ù…Ù†ØªØ¯ÙŠØ§Øª
            print("ğŸ’¬ Ø¬Ù…Ø¹ Ù…Ù† Ø§Ù„Ù…Ù†ØªØ¯ÙŠØ§Øª...")
            forum_data = self.collect_from_forums()
            all_data.extend(forum_data)
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØµÙÙŠØ©
            filtered_data = []
            for item in all_data:
                if self.filter_saudi_content(item['content']):
                    item['content_hash'] = hashlib.md5(
                        item['content'].encode('utf-8')
                    ).hexdigest()
                    filtered_data.append(item)
            
            print(f"âœ… ØªÙ… Ø¬Ù…Ø¹ {len(all_data)} Ø¹Ù†ØµØ±ØŒ ÙˆØªÙ…Øª ØªØµÙÙŠØ© {len(filtered_data)} Ø¹Ù†ØµØ± Ù…Ù†Ø§Ø³Ø¨")
            self.logger.info(f"Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {len(all_data)} Ø§Ù„ÙƒÙ„ØŒ {len(filtered_data)} Ù…ØµÙÙ‰")
            
            return filtered_data
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
            return []
    
    def filter_saudi_content(self, content: str) -> bool:
        """ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù†Ù‡ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©"""
        content_lower = content.lower()
        
        # ÙØ­Øµ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
        saudi_word_count = sum(1 for keyword in self.saudi_keywords 
                              if keyword.lower() in content_lower)
        
        # Ø¥Ø°Ø§ ÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø© Ø£Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
        return saudi_word_count >= 1
    
    def close(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª"""
        # Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§ØªØµØ§Ù„Ø§Øª Ù„Ù„Ø¥ØºÙ„Ø§Ù‚ ÙÙŠ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø¨Ø³Ø·Ø©
        self.logger.info("ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø¬Ø§Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")

if __name__ == "__main__":
    collector = SocialMediaCollector()
    
    try:
        # Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        collected_data = collector.collect_all_sources(max_per_source=10)
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print(f"\nğŸ“Š ØªÙ… Ø¬Ù…Ø¹ {len(collected_data)} Ø¹Ù†ØµØ± Ø¥Ø¬Ù…Ø§Ù„ÙŠØ§Ù‹")
        
        if collected_data:
            print("\nğŸ“„ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©:")
            for i, item in enumerate(collected_data[:5]):
                print(f"   {i+1}. [{item['platform']}] {item['content'][:60]}...")
                
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        with open(f"collected_sample_{datetime.now().strftime('%Y%m%d_%H%M')}.json", 'w', encoding='utf-8') as f:
            json.dump(collected_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ù„Ù JSON")
        
    finally:
        collector.close()